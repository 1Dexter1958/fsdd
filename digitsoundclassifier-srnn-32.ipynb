{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 1463427,
     "sourceType": "datasetVersion",
     "datasetId": 858190
    }
   ],
   "dockerImageVersionId": 31153,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import glob\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import subprocess\n",
    "import sys\n",
    "import math\n",
    "\n",
    "from collections import OrderedDict\n",
    "from typing import Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, confusion_matrix\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchaudio.transforms import AmplitudeToDB, MelSpectrogram\n",
    "\n",
    "AUDIO_DIR = \"/kaggle/input/free-spoken-digit-dataset-fsdd/recordings/\"\n",
    "\n",
    "# torch.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, targ_dir: str, transform=None, max_length: int = 16) -> None:\n",
    "        self.paths = list(pathlib.Path(targ_dir).glob(\"*.wav\"))\n",
    "        self.labels = [int(path.stem.split(\"_\")[0]) for path in self.paths]  # Convert labels to int\n",
    "        self.transform = transform\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def load_audio(self, index: int):\n",
    "        audio_path = self.paths[index]\n",
    "        waveform, sample_rate = torchaudio.load(audio_path, normalize=True)  # 关键修改\n",
    "        return waveform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, int]:\n",
    "        waveform = self.load_audio(index)\n",
    "        label = self.labels[index]\n",
    "\n",
    "        if self.transform:\n",
    "            # mel_spec_transform = MelSpectrogram(sample_rate=32000, n_fft=1024, hop_length=512, n_mels=128)\n",
    "            mel_spec_transform = MelSpectrogram(sample_rate=32000, n_fft=256, hop_length=128, n_mels=32)\n",
    "            amp2db_transform = AmplitudeToDB()\n",
    "            waveform = mel_spec_transform(waveform)\n",
    "            waveform = amp2db_transform(waveform)\n",
    "            \n",
    "            # 可选：对梅尔频谱进一步标准化\n",
    "            waveform = (waveform - waveform.mean()) / waveform.std()\n",
    "\n",
    "            # 长度调整\n",
    "            if waveform.shape[-1] < self.max_length:\n",
    "                padding = self.max_length - waveform.shape[-1]\n",
    "                waveform = torch.nn.functional.pad(waveform, (0, padding))\n",
    "            elif waveform.shape[-1] > self.max_length:\n",
    "                waveform = waveform[:, :, :self.max_length]\n",
    "\n",
    "        return waveform.squeeze(), label\n",
    "\n",
    "dataset = Dataset(AUDIO_DIR, transform=True)\n",
    "\n",
    "train_size = int(0.9 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=100)"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-05T09:38:59.459462Z",
     "iopub.execute_input": "2025-11-05T09:38:59.460136Z",
     "iopub.status.idle": "2025-11-05T09:38:59.485555Z",
     "shell.execute_reply.started": "2025-11-05T09:38:59.460103Z",
     "shell.execute_reply": "2025-11-05T09:38:59.484664Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "cpu\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "#SCELL TEST\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "lens = 0.5\n",
    "thresh = 0.875\n",
    "decay = 0.5 # decay constants(Leak)\n",
    "gamma = 0.5\n",
    "\n",
    "# surroguate_type = 'sigmoid'\n",
    "# surroguate_type = 'MG'\n",
    "# surroguate_type = 'slayer'\n",
    "surroguate_type = 'G'\n",
    "print('surroguate_type: ', surroguate_type)\n",
    "\n",
    "def gaussian(x, mu=0., sigma=0.5):\n",
    "    return torch.exp(-((x - mu) ** 2) / (2 * sigma ** 2)) / torch.sqrt(2 * torch.tensor(math.pi)) / sigma\n",
    "\n",
    "\n",
    "class AcFun_adp(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):  # input = memberane potential-threshold\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.gt(0).float()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):  # approximate the gradients\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "\n",
    "        scale = 6.0\n",
    "        height = 0.15\n",
    "        if surroguate_type == 'G':\n",
    "            temp = torch.exp(-(input ** 2) / (2 * lens ** 2)) / torch.sqrt(2 * torch.tensor(math.pi)) / lens\n",
    "        elif surroguate_type == 'MG':\n",
    "            temp = gaussian(input, mu=0., sigma=lens) * (1.0 + height) \\\n",
    "                   - gaussian(input, mu=lens, sigma=scale * lens) * height \\\n",
    "                   - gaussian(input, mu=-lens, sigma=scale * lens) * height\n",
    "        elif surroguate_type == 'linear':\n",
    "            temp = F.relu(1 - input.abs())\n",
    "        elif surroguate_type == 'slayer':\n",
    "            temp = torch.exp(-5 * input.abs())\n",
    "        elif surroguate_type == 'sigmoid':\n",
    "            temp = torch.exp(-input) / (1 + torch.exp(-input)) ** 2\n",
    "        return grad_input * temp.float() * gamma\n",
    "\n",
    "act_fun_adp = AcFun_adp.apply\n",
    "\n",
    "def LIF_mem_update(inputs, mem, spike):\n",
    "    # if(mem.shape[1] == 3):\n",
    "    #     print(mem*32768)\n",
    "    # mem = mem * decay * (1. - spike) + inputs\n",
    "    \n",
    "    n = float(2 ** 14)\n",
    "    mem = mem * decay * (1. - spike)\n",
    "    mem = torch.clip(torch.round(mem * n) / n , -14.99993896484375, 14.99993896484375)\n",
    "    # print(mem.shape)\n",
    "    # print(inputs.shape)\n",
    "    mem = mem  + inputs\n",
    "    mem = torch.clip(torch.round(mem * n) / n , -14.99993896484375, 14.99993896484375)\n",
    "\n",
    "    \n",
    "    temp_mem = mem - thresh\n",
    "    spike = act_fun_adp(temp_mem)\n",
    "    # spike = act_fun_adp(mem)\n",
    "\n",
    "    return mem, spike\n",
    "\n",
    "def Quantize(tensor, n_bit):\n",
    "    n = float(2 ** (n_bit-2))\n",
    "    # n = float(2 ** (n_bit-1))\n",
    "\n",
    "    return torch.clip(torch.round(tensor * n) / n , -1.99993896484375,1.99993896484375)\n",
    "    # return torch.clip(torch.round(tensor * n) / n , -0.9921875, 0.9921875)\n",
    "\n",
    "\n",
    "def Binarize(tensor):\n",
    "    return Quantize(tensor,16)\n",
    "\n",
    "\n",
    "class BinarizeLinear(nn.Linear):\n",
    "\n",
    "    def __init__(self, *kargs, **kwargs):\n",
    "        super(BinarizeLinear, self).__init__(*kargs, **kwargs)\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        if not hasattr(self.weight,'org'):\n",
    "            self.weight.org=self.weight.data.clone()\n",
    "        self.weight.data=Binarize(self.weight.org)\n",
    "        out = nn.functional.linear(input, self.weight)\n",
    "        if not self.bias is None:\n",
    "            self.bias.org=self.bias.data.clone()\n",
    "            out += self.bias.view(1, -1).expand_as(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "def Get_HexData(binary_tensor):\n",
    "    binary_tensor = binary_tensor.to(torch.int)\n",
    "\n",
    "    # 将张量展平为一维\n",
    "    flattened = binary_tensor.view(-1)\n",
    "\n",
    "    # 确保长度是4的倍数\n",
    "    if len(flattened) % 4 != 0:\n",
    "        padding = torch.zeros(4 - len(flattened) % 4, dtype=torch.int)\n",
    "        flattened = torch.cat((flattened, padding))\n",
    "\n",
    "    grouped = flattened.view(-1, 4)\n",
    "    grouped = torch.flip(grouped, dims=[-1])\n",
    "\n",
    "    # 将每4位二进制数转换为16进制字符\n",
    "    hex_data = []\n",
    "    for group in grouped:\n",
    "        binary_str = ''.join([str(bit.item()) for bit in group])  # 将4位二进制转换为字符串\n",
    "        hex_value = hex(int(binary_str, 2))[2:].upper()  # 转换为16进制并去掉前缀 '0x'，转为大写\n",
    "        hex_data.append(hex_value)\n",
    "\n",
    "    # 打印结果\n",
    "    # print(\"16进制数据:\", hex_data)\n",
    "    \n",
    "    # 倒序打印 hex_data\n",
    "    reversed_hex_data = hex_data[::-1]\n",
    "    print(\"倒序后的16进制数据:\", reversed_hex_data)\n",
    "\n",
    "print(\"Load LIF Done!!!!\")"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-05T09:39:03.191625Z",
     "iopub.execute_input": "2025-11-05T09:39:03.191999Z",
     "iopub.status.idle": "2025-11-05T09:39:03.210933Z",
     "shell.execute_reply.started": "2025-11-05T09:39:03.191970Z",
     "shell.execute_reply": "2025-11-05T09:39:03.209793Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "surroguate_type:  G\nLoad LIF Done!!!!\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "# GRUCELL TEST\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "class GRU(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, bias=False, dropout=0.0):\n",
    "        super(GRU, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # self.Encoding = nn.Linear(input_size, input_size, bias=bias)\n",
    "        self.RZGate = nn.Linear(input_size + hidden_size, hidden_size, bias=bias)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size, bias=bias)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        std = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for w in self.parameters():\n",
    "            w.data.uniform_(-std, std)\n",
    "\n",
    "    def forward(self, x, hidden_spike=None):\n",
    "        # 初始化隐藏状态\n",
    "        if hidden_spike is None:\n",
    "            hidden_spike = torch.zeros(x.size(0), self.hidden_size, device=x.device)\n",
    "        else:\n",
    "            # 确保hidden是三维的(num_layers, batch_size, hidden_size)\n",
    "            hidden_spike = hidden_spike.squeeze(0)\n",
    "\n",
    "        # 对序列中的每个时间步进行处理\n",
    "        Encoding_mem = Encoding_spike = (torch.zeros(x.size(0),x.size(1), x.size(2))).to(device)\n",
    "        \n",
    "        Encoding_mem, Encoding_spike = LIF_mem_update(x, Encoding_mem, Encoding_spike)\n",
    "        outputs = []\n",
    "        for t in range(x.size(1)):\n",
    "            # 获取当前时间步的输入\n",
    "            X_spike = Encoding_spike[:, t, :]\n",
    "            # X_spike = x[:, t, :]\n",
    "            # 计算门控\n",
    "            # print(xt)\n",
    "            # print(hidden_spike)\n",
    "            # xt = EncodingBinarize(xt)\n",
    "            # X_spike = self.relu(self.Encoding(xt))\n",
    "            \n",
    "            x_in = torch.cat((X_spike, hidden_spike), dim=1).to(device)\n",
    "            hidden_spike = torch.sigmoid(self.RZGate(x_in))\n",
    "\n",
    "            # print(hidden_spike)\n",
    "            o_input = self.h2o(hidden_spike)\n",
    "\n",
    "            # print(output_spike_sum)\n",
    "\n",
    "        # print(h)\n",
    "        return o_input\n",
    "\n",
    "\n",
    "print(\"Network!\")\n",
    "# 使用示例\n",
    "# 假设输入序列x的形状是(batch_size, seq_length, input_size)\n",
    "# 初始化GRUCell\n",
    "gru_cell = GRU(input_size=10, hidden_size=20, output_size = 2).to(device)\n",
    "\n",
    "# 假设的输入序列\n",
    "x = torch.randn(5, 3, 10).to(device)  # batch_size=5, seq_length=3, input_size=10\n",
    "\n",
    "# 调用GRUCell，得到所有时间步的隐藏状态\n",
    "out= gru_cell(x)\n",
    "\n",
    "# h的形状是(batch_size, seq_length, hidden_size)\n",
    "print(out.shape)  # 应该输出torch.Size([5, 2])\n"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-05T09:39:08.115409Z",
     "iopub.execute_input": "2025-11-05T09:39:08.116165Z",
     "iopub.status.idle": "2025-11-05T09:39:08.245541Z",
     "shell.execute_reply.started": "2025-11-05T09:39:08.116127Z",
     "shell.execute_reply": "2025-11-05T09:39:08.244725Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Network!\ntorch.Size([5, 2])\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "# GRUCELL TEST\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "class SGRU(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, bias=False, dropout=0.0):\n",
    "        super(SGRU, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # self.RZGate = BinarizeLinear(input_size + hidden_size, hidden_size, bias=bias)\n",
    "        # self.h2o = BinarizeLinear(hidden_size, output_size, bias=bias)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.RZGate = nn.Linear(input_size + hidden_size, hidden_size, bias=bias)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size, bias=bias)\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        std = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for w in self.parameters():\n",
    "            w.data.uniform_(-std, std)\n",
    "\n",
    "    def forward(self, x, hidden_spike=None):\n",
    "        # 初始化隐藏状态\n",
    "        if hidden_spike is None:\n",
    "            hidden_spike = torch.zeros(x.size(0), self.hidden_size, device=x.device)\n",
    "        else:\n",
    "            # 确保hidden是三维的(num_layers, batch_size, hidden_size)\n",
    "            hidden_spike = hidden_spike.squeeze(0)\n",
    "\n",
    "        # 对序列中的每个时间步进行处理\n",
    "        Encoding_mem = Encoding_spike = (torch.zeros(x.size(0),x.size(1), x.size(2))).to(device)\n",
    "        RZgate_mem = RZgate_spike = torch.zeros(x.size(0), self.hidden_size, device=x.device)\n",
    "        \n",
    "        Encoding_mem, Encoding_spike = LIF_mem_update(x, Encoding_mem, Encoding_spike)\n",
    "        outputs = []\n",
    "        for t in range(x.size(1)):\n",
    "            # 获取当前时间步的输入\n",
    "            X_spike = Encoding_spike[:, t, :]\n",
    "            # X_spike = x[:, t, :]\n",
    "            # 计算门控\n",
    "            # print(xt)\n",
    "            # print(hidden_spike)\n",
    "            # xt = EncodingBinarize(xt)\n",
    "            # X_spike = self.relu(self.Encoding(xt))\n",
    "            \n",
    "            x_in = torch.cat((X_spike, hidden_spike), dim=1)\n",
    "\n",
    "            RZgate_x =self.RZGate(x_in)\n",
    "            RZgate_mem, RZgate_spike = LIF_mem_update(RZgate_x, RZgate_mem, RZgate_spike)\n",
    "\n",
    "            # print(hidden_spike)\n",
    "            hidden_spike = RZgate_spike\n",
    "            o_input = self.h2o(hidden_spike)\n",
    "\n",
    "            # print(output_spike_sum)\n",
    "\n",
    "        # print(h)\n",
    "        return o_input\n",
    "\n",
    "\n",
    "print(\"Network!\")\n",
    "# 使用示例\n",
    "# 假设输入序列x的形状是(batch_size, seq_length, input_size)\n",
    "# 初始化GRUCell\n",
    "gru_cell = SGRU(input_size=10, hidden_size=20, output_size = 2).to(device)\n",
    "\n",
    "# 假设的输入序列\n",
    "x = torch.randn(5, 3, 10).to(device)  # batch_size=5, seq_length=3, input_size=10\n",
    "\n",
    "# 调用GRUCell，得到所有时间步的隐藏状态\n",
    "out= gru_cell(x)\n",
    "\n",
    "# h的形状是(batch_size, seq_length, hidden_size)\n",
    "print(out.shape)  # 应该输出torch.Size([5, 2])\n"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-05T09:39:12.606792Z",
     "iopub.execute_input": "2025-11-05T09:39:12.607130Z",
     "iopub.status.idle": "2025-11-05T09:39:12.623121Z",
     "shell.execute_reply.started": "2025-11-05T09:39:12.607104Z",
     "shell.execute_reply": "2025-11-05T09:39:12.622270Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Network!\ntorch.Size([5, 2])\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "# GRUCELL TEST\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "class LSGRU(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, bias=False, dropout=0.0):\n",
    "        super(LSGRU, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.RZGate = BinarizeLinear(input_size + hidden_size, hidden_size, bias=bias)\n",
    "        self.h2o = BinarizeLinear(hidden_size, output_size, bias=bias)\n",
    "        \n",
    "\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        std = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for w in self.parameters():\n",
    "            w.data.uniform_(-std, std)\n",
    "\n",
    "    def forward(self, x, hidden_spike=None):\n",
    "        # 初始化隐藏状态\n",
    "        if hidden_spike is None:\n",
    "            hidden_spike = torch.zeros(x.size(0), self.hidden_size, device=x.device)\n",
    "        else:\n",
    "            # 确保hidden是三维的(num_layers, batch_size, hidden_size)\n",
    "            hidden_spike = hidden_spike.squeeze(0)\n",
    "\n",
    "        # 对序列中的每个时间步进行处理\n",
    "        Encoding_mem = Encoding_spike = (torch.zeros(x.size(0),x.size(1), x.size(2))).to(device)\n",
    "        RZgate_mem = RZgate_spike = torch.zeros(x.size(0), self.hidden_size, device=x.device)\n",
    "        \n",
    "        Encoding_mem, Encoding_spike = LIF_mem_update(x, Encoding_mem, Encoding_spike)\n",
    "        outputs = []\n",
    "        for t in range(x.size(1)):\n",
    "            # 获取当前时间步的输入\n",
    "            X_spike = Encoding_spike[:, t, :]\n",
    "            # X_spike = x[:, t, :]\n",
    "            # 计算门控\n",
    "            # print(xt)\n",
    "            # print(hidden_spike)\n",
    "            # xt = EncodingBinarize(xt)\n",
    "            # X_spike = self.relu(self.Encoding(xt))\n",
    "            \n",
    "            x_in = torch.cat((X_spike, hidden_spike), dim=1)\n",
    "            # print(\"input Spiking!!!\")\n",
    "            # print(hidden_spike)\n",
    "            # print(x_in)\n",
    "\n",
    "            RZgate_x =self.RZGate(x_in)\n",
    "            RZgate_mem, RZgate_spike = LIF_mem_update(RZgate_x, RZgate_mem, RZgate_spike)\n",
    "\n",
    "            # print(\"hidden Spiking!!!\")\n",
    "            # print(RZgate_spike)\n",
    "            hidden_spike = RZgate_spike\n",
    "            o_input = self.h2o(hidden_spike)\n",
    "\n",
    "            # print(output_spike_sum)\n",
    "\n",
    "        # print(h)\n",
    "        return o_input\n",
    "\n",
    "\n",
    "print(\"Network!\")\n",
    "# 使用示例\n",
    "# 假设输入序列x的形状是(batch_size, seq_length, input_size)\n",
    "# 初始化GRUCell\n",
    "gru_cell = LSGRU(input_size=10, hidden_size=20, output_size = 2).to(device)\n",
    "\n",
    "# 假设的输入序列\n",
    "x = torch.randn(5, 3, 10).to(device)  # batch_size=5, seq_length=3, input_size=10\n",
    "\n",
    "# 调用GRUCell，得到所有时间步的隐藏状态\n",
    "out= gru_cell(x)\n",
    "\n",
    "# h的形状是(batch_size, seq_length, hidden_size)\n",
    "print(out.shape)  # 应该输出torch.Size([5, 2])\n"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-05T09:39:15.544331Z",
     "iopub.execute_input": "2025-11-05T09:39:15.545607Z",
     "iopub.status.idle": "2025-11-05T09:39:15.568180Z",
     "shell.execute_reply.started": "2025-11-05T09:39:15.545561Z",
     "shell.execute_reply": "2025-11-05T09:39:15.567189Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Network!\ntorch.Size([5, 2])\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "# GRUCELL TEST\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "class OLSGRU(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, bias=False, dropout=0.0):\n",
    "        super(OLSGRU, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.RZGate = BinarizeLinear(input_size + hidden_size, hidden_size, bias=bias)\n",
    "        self.h2o = BinarizeLinear(hidden_size, output_size, bias=bias)\n",
    "        \n",
    "\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        std = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for w in self.parameters():\n",
    "            w.data.uniform_(-std, std)\n",
    "\n",
    "    def forward(self, x, hidden_spike=None):\n",
    "        # 初始化隐藏状态\n",
    "        if hidden_spike is None:\n",
    "            hidden_spike = torch.zeros(x.size(0), self.hidden_size, device=x.device)\n",
    "        else:\n",
    "            # 确保hidden是三维的(num_layers, batch_size, hidden_size)\n",
    "            hidden_spike = hidden_spike.squeeze(0)\n",
    "\n",
    "        # 对序列中的每个时间步进行处理\n",
    "        Encoding_mem = Encoding_spike = (torch.zeros(x.size(0),x.size(1), x.size(2))).to(device)\n",
    "        RZgate_mem = RZgate_spike = torch.zeros(x.size(0), self.hidden_size, device=x.device)\n",
    "        \n",
    "        Encoding_mem, Encoding_spike = LIF_mem_update(x, Encoding_mem, Encoding_spike)\n",
    "        \n",
    "        o_input = Encoding_spike\n",
    "\n",
    "            # print(output_spike_sum)\n",
    "\n",
    "        # print(h)\n",
    "        return o_input\n",
    "\n",
    "\n",
    "print(\"Network!\")\n",
    "# 使用示例\n",
    "# 假设输入序列x的形状是(batch_size, seq_length, input_size)\n",
    "# 初始化GRUCell\n",
    "gru_cell = OLSGRU(input_size=10, hidden_size=20, output_size = 2).to(device)\n",
    "\n",
    "# 假设的输入序列\n",
    "x = torch.randn(5, 3, 10).to(device).to(device)  # batch_size=5, seq_length=3, input_size=10\n",
    "\n",
    "# 调用GRUCell，得到所有时间步的隐藏状态\n",
    "out= gru_cell(x)\n",
    "\n",
    "# h的形状是(batch_size, seq_length, hidden_size)\n",
    "print(out.shape)  # 应该输出torch.Size([5, 2])\n"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-05T12:01:13.818618Z",
     "iopub.execute_input": "2025-11-05T12:01:13.818948Z",
     "iopub.status.idle": "2025-11-05T12:01:13.832301Z",
     "shell.execute_reply.started": "2025-11-05T12:01:13.818924Z",
     "shell.execute_reply": "2025-11-05T12:01:13.831268Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Network!\ntorch.Size([5, 3, 10])\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # Create a matrix of shape (max_len, d_model)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # Apply sine to even indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # Apply cosine to odd indices\n",
    "        pe = pe.unsqueeze(0)  # Add a batch dimension (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, time_steps, d_model)\n",
    "        # Add positional encoding to the input embeddings\n",
    "        # print(self.pe.device)\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class SpeechTransformer(nn.Module):\n",
    "    def __init__(self, num_classes=10, d_model=32, nhead=4, num_layers=4):\n",
    "        super(SpeechTransformer, self).__init__()\n",
    "        # self.input_layer = nn.Linear(128, d_model)  # Map mel bins to d_model\n",
    "        self.positional_encoding = PositionalEncoding(d_model)  # Add positional encoding\n",
    "        \n",
    "        # self.classifier = GRU(d_model, 64, 10)\n",
    "        # self.classifier = SGRU(d_model, 64, 10)\n",
    "        # self.classifier = LSGRU(d_model, 64, 10)\n",
    "        self.classifier = OLSGRU(d_model, 64, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        # x shape: (batch_size, mel_bins, time_steps)\n",
    "        x = x.transpose(1, 2)  # (batch_size, time_steps, mel_bins)\n",
    "        # x = self.input_layer(x)  # Map to d_model\n",
    "        x = self.positional_encoding(x)  # Add positional encoding\n",
    "        # print(x[0][0])\n",
    "        # xx\n",
    "        # print(x.shape)\n",
    "        # x = self.transformer_encoder(x)  # Pass through transformer encoder\n",
    "        # x = x.mean(dim=1)  # Aggregate time dimension (average over time)\n",
    "        \n",
    "        x = self.classifier(x)  # Final classifier\n",
    "        \n",
    "        # _, x_, s_ = self.classifier(x)  # Final classifier\n",
    "        # print(x.shape)\n",
    "        # xx\n",
    "        return x"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-05T12:01:23.383814Z",
     "iopub.execute_input": "2025-11-05T12:01:23.384223Z",
     "iopub.status.idle": "2025-11-05T12:01:23.393548Z",
     "shell.execute_reply.started": "2025-11-05T12:01:23.384194Z",
     "shell.execute_reply": "2025-11-05T12:01:23.392638Z"
    }
   },
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "# Prepare for saving models\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# torch.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model = SpeechTransformer().to(device)\n",
    "model.load_state_dict(torch.load(\"/kaggle/working/models/srnn_best_model.pth\"))\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = StepLR(optimizer, step_size=80, gamma=0.1)\n",
    "\n",
    "# Training\n",
    "epochs = 5\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "best_val_accuracy = 0.0  # Track the best validation accuracy\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        # print(inputs[0][0])\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # print(inputs.device, labels.device)  \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        # print(outputs.device)\n",
    "        # xx\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            # print(inputs[0][0])\n",
    "            # xx\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    # Calculate metrics\n",
    "    train_loss_avg = train_loss / len(train_loader)\n",
    "    val_loss_avg = val_loss / len(test_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    train_loss_list.append(train_loss_avg)\n",
    "    val_loss_list.append(val_loss_avg)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1}, Train Loss: {train_loss_avg:.4f}, Val Loss: {val_loss_avg:.4f}, Val Accuracy: {val_accuracy:.2f}%\"\n",
    "    )\n",
    "\n",
    "    # Save the model if it's the best so far\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        torch.save(model.state_dict(), os.path.join(\"models\", \"lsrnn_best_model.pth\"))\n",
    "        print(f\"Best model saved with Val Accuracy: {val_accuracy:.2f}%\")"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-05T12:00:08.862455Z",
     "iopub.execute_input": "2025-11-05T12:00:08.863358Z",
     "iopub.status.idle": "2025-11-05T12:01:07.953912Z",
     "shell.execute_reply.started": "2025-11-05T12:00:08.863314Z",
     "shell.execute_reply": "2025-11-05T12:01:07.953043Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "cpu\nEpoch 1, Train Loss: 1.5346, Val Loss: 1.6366, Val Accuracy: 43.00%\nBest model saved with Val Accuracy: 43.00%\nEpoch 2, Train Loss: 1.5346, Val Loss: 1.6366, Val Accuracy: 43.00%\nEpoch 3, Train Loss: 1.5346, Val Loss: 1.6366, Val Accuracy: 43.00%\nEpoch 4, Train Loss: 1.5346, Val Loss: 1.6366, Val Accuracy: 43.00%\nEpoch 5, Train Loss: 1.5346, Val Loss: 1.6366, Val Accuracy: 43.00%\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 初始化存储容器\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "Tran_Model = SpeechTransformer()\n",
    "Tran_Model.load_state_dict(torch.load(\"/kaggle/working/models/lsrnn_best_model.pth\"))\n",
    "\n",
    "# 确保保存路径存在\n",
    "os.makedirs(\"save_dir\", exist_ok=True)\n",
    "\n",
    "# 训练循环\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # for inputs, labels in train_loader:\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        feature = Tran_Model(inputs)\n",
    "        # print(feature[0][0])\n",
    "        # xxx\n",
    "        # 转换为 NumPy 并累积\n",
    "        feature_np = feature.cpu().detach().numpy()\n",
    "        labels_np = labels.cpu().detach().numpy()\n",
    "        \n",
    "        all_features.append(feature_np)\n",
    "        all_labels.append(labels_np)\n",
    "\n",
    "# 合并并保存\n",
    "all_features = np.concatenate(all_features, axis=0)\n",
    "all_labels = np.concatenate(all_labels, axis=0)\n",
    "print(all_features.shape)\n",
    "print(all_labels.shape)\n",
    "print(all_labels)\n",
    "# xxx\n",
    "# np.save(\"save_dir/FSDD_train_data.npy\", all_features)\n",
    "# np.save(\"save_dir/FSDD_train_label.npy\", all_labels)\n",
    "np.save(\"save_dir/FSDD_test_data.npy\", all_features)\n",
    "np.save(\"save_dir/FSDD_test_label.npy\", all_labels)"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-05T12:01:30.325526Z",
     "iopub.execute_input": "2025-11-05T12:01:30.325812Z",
     "iopub.status.idle": "2025-11-05T12:01:31.442940Z",
     "shell.execute_reply.started": "2025-11-05T12:01:30.325792Z",
     "shell.execute_reply": "2025-11-05T12:01:31.441863Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "(300, 16, 32)\n(300,)\n[3 1 6 2 4 1 0 2 0 8 9 8 0 3 3 7 2 4 4 8 6 8 4 7 6 4 0 9 1 4 5 6 1 0 8 7 0\n 0 6 5 4 9 0 5 5 6 8 5 6 9 3 5 3 7 9 5 0 8 3 3 3 6 6 7 2 3 4 1 6 1 1 7 3 1\n 2 2 0 7 5 5 6 9 7 4 6 0 3 1 7 5 9 9 9 6 2 4 5 6 2 4 6 3 6 7 5 1 6 5 0 4 5\n 4 7 2 9 8 1 3 3 3 9 2 1 8 4 1 4 7 4 3 9 9 4 2 9 4 5 6 7 2 8 1 8 8 3 8 8 5\n 8 3 3 8 2 2 9 7 9 3 2 2 8 8 4 6 9 8 9 5 7 3 3 5 6 4 1 0 7 3 9 3 4 7 2 4 4\n 4 9 4 0 5 9 6 4 5 3 3 3 5 3 3 0 8 2 6 3 9 5 1 4 5 8 1 7 2 7 8 4 9 9 1 0 7\n 9 7 7 6 7 0 1 3 8 9 8 9 1 2 8 5 3 8 5 6 4 1 1 1 6 7 2 1 9 3 0 7 9 7 5 4 1\n 6 7 1 2 5 2 6 1 3 9 0 0 2 9 6 7 8 5 9 7 7 6 6 9 7 2 3 2 0 7 3 3 3 5 8 0 7\n 0 0 9 6]\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "source": [
    "###------------------------------------------------TEST--------------------------------------------"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-05T12:01:45.316782Z",
     "iopub.execute_input": "2025-11-05T12:01:45.317132Z",
     "iopub.status.idle": "2025-11-05T12:01:45.321439Z",
     "shell.execute_reply.started": "2025-11-05T12:01:45.317106Z",
     "shell.execute_reply": "2025-11-05T12:01:45.320611Z"
    }
   },
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "class CustomTestDataset(Dataset):\n",
    "    def __init__(self, data_path, label_path, transform=None):\n",
    "        self.data = np.load(data_path)  # 加载数据文件 \n",
    "        self.labels = np.load(label_path)  # 加载标签文件 \n",
    "        self.transform = transform  # 可选的数据预处理（如转为Tensor）\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)  # 确保数据与标签数量一致 \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)  # 应用预处理（如归一化）\n",
    "        return sample, label\n",
    "\n",
    "\n",
    "# 定义预处理流程\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # 将numpy数组转为Tensor，并自动归一化到[0,1] [[1]]\n",
    "    # transforms.Normalize(mean=[0.5], std=[0.5])  # 可选标准化\n",
    "])\n",
    "\n",
    "# 初始化Dataset\n",
    "test_dataset = CustomTestDataset(\n",
    "    data_path='/kaggle/working/save_dir/FSDD_test_data.npy',\n",
    "    label_path='/kaggle/working/save_dir/FSDD_test_label.npy',\n",
    "    transform=test_transform\n",
    ")\n",
    "\n",
    "# 创建DataLoader\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=64,  # 按需设置批次大小\n",
    "    shuffle=False,  # 测试时通常不洗牌 \n",
    "    num_workers=2   # 多进程加载加速\n",
    ")"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-05T12:01:50.719745Z",
     "iopub.execute_input": "2025-11-05T12:01:50.720129Z",
     "iopub.status.idle": "2025-11-05T12:01:51.597384Z",
     "shell.execute_reply.started": "2025-11-05T12:01:50.720097Z",
     "shell.execute_reply": "2025-11-05T12:01:51.596380Z"
    }
   },
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "source": [
    "def Get_HexData(binary_tensor):\n",
    "    binary_tensor = binary_tensor.to(torch.int)\n",
    "\n",
    "    # 将张量展平为一维\n",
    "    flattened = binary_tensor.view(-1)\n",
    "\n",
    "    # 确保长度是4的倍数\n",
    "    if len(flattened) % 4 != 0:\n",
    "        padding = torch.zeros(4 - len(flattened) % 4, dtype=torch.int)\n",
    "        flattened = torch.cat((flattened, padding))\n",
    "\n",
    "    grouped = flattened.view(-1, 4)\n",
    "    grouped = torch.flip(grouped, dims=[-1])\n",
    "\n",
    "    # 将每4位二进制数转换为16进制字符\n",
    "    hex_data = []\n",
    "    for group in grouped:\n",
    "        binary_str = ''.join([str(bit.item()) for bit in group])  # 将4位二进制转换为字符串\n",
    "        hex_value = hex(int(binary_str, 2))[2:].upper()  # 转换为16进制并去掉前缀 '0x'，转为大写\n",
    "        hex_data.append(hex_value)\n",
    "\n",
    "    # 打印结果\n",
    "    # print(\"16进制数据:\", hex_data)\n",
    "    \n",
    "    # 倒序打印 hex_data\n",
    "    reversed_hex_data = hex_data[::-1]\n",
    "    print(\"倒序后的16进制数据:\", reversed_hex_data)"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-05T12:01:55.951385Z",
     "iopub.execute_input": "2025-11-05T12:01:55.952070Z",
     "iopub.status.idle": "2025-11-05T12:01:55.959396Z",
     "shell.execute_reply.started": "2025-11-05T12:01:55.952028Z",
     "shell.execute_reply": "2025-11-05T12:01:55.958279Z"
    }
   },
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "source": [
    "# 在创建Dataset之前检查\n",
    "data = np.load('/kaggle/working/save_dir/FSDD_test_data.npy')\n",
    "labels = np.load('/kaggle/working/save_dir/FSDD_test_label.npy')\n",
    "\n",
    "print(f\"整体数据形状: {data.shape}\")\n",
    "print(f\"标签形状: {labels.shape}\")\n",
    "print(f\"第一个样本形状: {data[0].shape}\")\n",
    "print(f\"第一个样本维度: {data[0].ndim}\")"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-05T12:01:58.504660Z",
     "iopub.execute_input": "2025-11-05T12:01:58.505010Z",
     "iopub.status.idle": "2025-11-05T12:01:58.512700Z",
     "shell.execute_reply.started": "2025-11-05T12:01:58.504979Z",
     "shell.execute_reply": "2025-11-05T12:01:58.511321Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "整体数据形状: (300, 16, 32)\n标签形状: (300,)\n第一个样本形状: (16, 32)\n第一个样本维度: 2\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "# Prepare for saving models\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "from collections import OrderedDict\n",
    "\n",
    "# torch.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model = LSGRU(input_size=32, hidden_size=64, output_size=10).to(device)\n",
    "# model.load_state_dict(torch.load(\"/kaggle/input/fffffsdd/FSDD_SRNN/best_model2 (1).pth\"))\n",
    "# 加载原始 state_dict\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "pretrained_dict = torch.load('/kaggle/working/models/lsrnn_best_model.pth')\n",
    "# 创建新字典，过滤并重命名键\n",
    "new_state_dict = OrderedDict()\n",
    "\n",
    "for k, v in pretrained_dict.items():\n",
    "    if k.startswith(\"classifier.\"):\n",
    "        # 移除 \"classifier.\" 前缀\n",
    "        name = k.replace(\"classifier.\", \"\", 1)\n",
    "        new_state_dict[name] = v\n",
    "    # 可添加其他条件过滤不需要的键（如 positional_encoding.pe）\n",
    "    elif k == \"positional_encoding.pe\":\n",
    "        continue  # 直接跳过此键\n",
    "    else:\n",
    "        new_state_dict[k] = v  # 保留其他键\n",
    "\n",
    "# 加载调整后的 state_dict\n",
    "model.load_state_dict(new_state_dict, strict=False)\n",
    "print(model)\n",
    "# xxx\n",
    "val_loss_list = []\n",
    "model.eval()\n",
    "val_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        print(inputs.shape)\n",
    "        # xx\n",
    "        inputs = inputs.squeeze(dim=1) \n",
    "        print(inputs.shape)\n",
    "        print(labels.shape)\n",
    "        # print(inputs[0][0])\n",
    "        # print(labels)\n",
    "        # xxx\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        # outputs = torch.clamp(outputs, min= 0)\n",
    "        n = float(2 ** 3)\n",
    "        outputs = torch.clip(torch.round(outputs * n) / n , -14.875, 14.875)\n",
    "        print(outputs.shape)\n",
    "\n",
    "        # 非全负数的个数\n",
    "        # non_negative_mask = outputs >= 0\n",
    "        # 沿行方向（dim=1）判断是否存在至少一个非负元素\n",
    "        # has_non_negative = torch.any(non_negative_mask, dim=1)\n",
    "        # 统计满足条件的行数\n",
    "        # count = torch.sum(has_non_negative).item()\n",
    "        # print(count)\n",
    "\n",
    "        # 判断每行是否全为负数\n",
    "        # all_negative_mask = torch.all(outputs < 0, dim=1)  # 使用dim=1沿行方向检查\n",
    "        # 获取全负数行的索引\n",
    "        # invalid_indices = torch.nonzero(all_negative_mask, as_tuple=True)[0].tolist()\n",
    "        # print(\"全负数行的索引：\", invalid_indices)\n",
    "        \n",
    "        # print(\"------------------\")\n",
    "        # for i in range(64):\n",
    "        #     print(outputs[i])\n",
    "        print(outputs.max())\n",
    "        print(outputs.min())\n",
    "        loss = criterion(outputs, labels)\n",
    "        val_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "# Calculate metrics\n",
    "val_loss_avg = val_loss / len(test_loader)\n",
    "val_accuracy = 100 * correct / total\n",
    "val_loss_list.append(val_loss_avg)\n",
    "\n",
    "print(f\"Val Loss: {val_loss_avg:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
    "torch.save(model.state_dict(), os.path.join(\"models\", \"best_model.pth\"))\n"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-05T12:02:05.734825Z",
     "iopub.execute_input": "2025-11-05T12:02:05.735204Z",
     "iopub.status.idle": "2025-11-05T12:02:05.958238Z",
     "shell.execute_reply.started": "2025-11-05T12:02:05.735176Z",
     "shell.execute_reply": "2025-11-05T12:02:05.957269Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "cpu\nLSGRU(\n  (RZGate): BinarizeLinear(in_features=96, out_features=64, bias=False)\n  (h2o): BinarizeLinear(in_features=64, out_features=10, bias=False)\n  (dropout_layer): Dropout(p=0.0, inplace=False)\n)\ntorch.Size([64, 1, 16, 32])\ntorch.Size([64, 16, 32])\ntorch.Size([64])\ntorch.Size([64, 10])\ntensor(5.5000)\ntensor(-6.)\ntorch.Size([64, 1, 16, 32])\ntorch.Size([64, 16, 32])\ntorch.Size([64])\ntorch.Size([64, 10])\ntensor(4.6250)\ntensor(-6.)\ntorch.Size([64, 1, 16, 32])\ntorch.Size([64, 16, 32])\ntorch.Size([64])\ntorch.Size([64, 10])\ntensor(6.6250)\ntensor(-6.)\ntorch.Size([64, 1, 16, 32])\ntorch.Size([64, 16, 32])\ntorch.Size([64])\ntorch.Size([64, 10])\ntensor(4.5000)\ntensor(-6.5000)\ntorch.Size([44, 1, 16, 32])\ntorch.Size([44, 16, 32])\ntorch.Size([44])\ntorch.Size([44, 10])\ntensor(4.7500)\ntensor(-6.1250)\nVal Loss: 1.6384, Val Accuracy: 42.33%\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "source": [
    "print(model.RZGate.weight.shape)\n",
    "print(model.h2o.weight.shape)"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-05T12:02:20.752936Z",
     "iopub.execute_input": "2025-11-05T12:02:20.753272Z",
     "iopub.status.idle": "2025-11-05T12:02:20.758852Z",
     "shell.execute_reply.started": "2025-11-05T12:02:20.753246Z",
     "shell.execute_reply": "2025-11-05T12:02:20.757940Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "torch.Size([64, 96])\ntorch.Size([10, 64])\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# 加载权重文件\n",
    "state_dict = torch.load(\"/kaggle/working/models/best_model.pth\")\n",
    "\n",
    "# 遍历字典中的每个权重\n",
    "for name, weight in state_dict.items():\n",
    "    # 将点分隔的层名转换为下划线分隔的文件名\n",
    "    filename = \"/kaggle/working/\" + name.replace('.', '_') + '.pth'\n",
    "    # 创建一个包含名称和权重的字典\n",
    "    weight_dict = {name: weight}\n",
    "    # 保存权重到文件\n",
    "    torch.save(weight_dict, filename)\n",
    "    print(f'Saved {name} to {filename}')\n",
    "    print(\"shape is:\",weight.shape)"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-05T12:02:25.948193Z",
     "iopub.execute_input": "2025-11-05T12:02:25.948502Z",
     "iopub.status.idle": "2025-11-05T12:02:25.958156Z",
     "shell.execute_reply.started": "2025-11-05T12:02:25.948480Z",
     "shell.execute_reply": "2025-11-05T12:02:25.957153Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Saved RZGate.weight to /kaggle/working/RZGate_weight.pth\nshape is: torch.Size([64, 96])\nSaved h2o.weight to /kaggle/working/h2o_weight.pth\nshape is: torch.Size([10, 64])\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "source": [
    "os.makedirs(\"Weight_npy\", exist_ok=True)\n",
    "load_dict = torch.load(r'/kaggle/working/RZGate_weight.pth',map_location='cpu')\n",
    "# load_dict = torch.load(r'/kaggle/working/h2o_weight.pth',map_location='cpu')\n",
    "# print(load_dict)\n",
    "i = 0\n",
    "\n",
    "pre = 1\n",
    "\n",
    "for k, v in load_dict.items():\n",
    "    print(type(v))\n",
    "    a = v.cpu().numpy()\n",
    "    print(a.shape)\n",
    "    # print(a)\n",
    "    print(type(a))\n",
    "    if k[-6:] == 'weight':\n",
    "        print(\"save weight\")\n",
    "        np.save(r\"Weight_npy/{}.npy\".format(k), a)"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-05T12:02:34.559534Z",
     "iopub.execute_input": "2025-11-05T12:02:34.559896Z",
     "iopub.status.idle": "2025-11-05T12:02:34.570052Z",
     "shell.execute_reply.started": "2025-11-05T12:02:34.559846Z",
     "shell.execute_reply": "2025-11-05T12:02:34.569015Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "<class 'torch.Tensor'>\n(64, 96)\n<class 'numpy.ndarray'>\nsave weight\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "source": [
    "def bin2hex_fan(x):\n",
    "# 调用input216bit函数，函数将输入的数转换为16位的二进制字符串。\n",
    "  x = input216bit(x)\n",
    "\n",
    "  z = x\n",
    "  new_hex = []\n",
    "# 循环4次，因为16位二进制可以分成4组，每组4位。\n",
    "# 将每组4位二进制转换为十六进制，并移除前缀0x。\n",
    "# 将转换后的十六进制字符添加到列表中。\n",
    "# 将列表中的十六进制字符连接成字符串，并转换为大写。\n",
    "  for i in range(4):\n",
    "      temp = hex(int(z[i*4:(i+1)*4], 2)).replace(\"0x\", \"\")\n",
    "      new_hex.append(temp)\n",
    "  return \"\".join(new_hex).upper()\n",
    "\n",
    "def dec2bin(x):\n",
    "# 这个函数将浮点数的小数部分转换为二进制表示，最多保留15位。\n",
    "# 移除x的整数部分，仅保留小数部分。\n",
    "  int_X = int(x)\n",
    "  x -= int(x)\n",
    "  bins = []\n",
    "# 当x不为0且二进制位数少于15位时，继续循环。\n",
    "# *2相对于进行左移位，将小数变成二进制的整数，判断是否为1，之后再减去该值，继续后续的循环操作\n",
    "# 返回二进制字符串，并在前面补足0，以确保总共有15位。\n",
    "  bins.append(str(1) if int_X >= 1. else str(0))\n",
    "  while x and len(bins)<15:\n",
    "    x *= 2\n",
    "    bins.append(str(1) if x>=1. else str(0))\n",
    "    x -= int(x)\n",
    "  return \"\".join(bins) + (15-len(bins))*str(0)\n",
    "\n",
    "# 将整数转换为3位的二进制表示，返回格式化的字符串，确保长度为3位。\n",
    "def int2bin(x):\n",
    "  return '{0:03b}'.format(x)\n",
    "\n",
    "def input216bit(x):\n",
    "# 函数将输入的数转换为16位的二进制字符串。\n",
    "# 取x的绝对值。\n",
    "  new_x = abs(x)\n",
    "\n",
    "  # int_bin = int2bin(int(new_x))\n",
    "# 调用dec2bin函数，获取x的小数部分二进制表示。（15位小数）\n",
    "  dec_bin = dec2bin(new_x)\n",
    "\n",
    "# 判断他的正负号\n",
    "  if x > 0:\n",
    "    return \"0\" + dec_bin\n",
    "  elif x == 0:\n",
    "    return  16 * \"0\"\n",
    "  else:\n",
    "    return \"1\" + dec_bin\n",
    "\n",
    "test1 = 1.99993896484375\n",
    "quan1 = bin2hex_fan(test1)\n",
    "print(test1)\n",
    "print(quan1)\n",
    "\n",
    "test1 = 0.99993896484375\n",
    "quan1 = bin2hex_fan(test1)\n",
    "print(test1)\n",
    "print(quan1)\n",
    "\n",
    "test1 = -1.99993896484375\n",
    "quan1 = bin2hex_fan(test1)\n",
    "print(test1)\n",
    "print(quan1)\n",
    "\n",
    "test1 = -0.99993896484375\n",
    "quan1 = bin2hex_fan(test1)\n",
    "print(test1)\n",
    "print(quan1)"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-05T12:02:51.662796Z",
     "iopub.execute_input": "2025-11-05T12:02:51.663226Z",
     "iopub.status.idle": "2025-11-05T12:02:51.675145Z",
     "shell.execute_reply.started": "2025-11-05T12:02:51.663201Z",
     "shell.execute_reply": "2025-11-05T12:02:51.673715Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "1.99993896484375\n7FFF\n0.99993896484375\n3FFF\n-1.99993896484375\nFFFF\n-0.99993896484375\nBFFF\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "source": [
    "FC1_Weight = np.load(r'/kaggle/working/Weight_npy/RZGate.weight.npy').T\n",
    "FC3_Weight = np.load(r'/kaggle/working/Weight_npy/h2o.weight.npy').T\n",
    "\n",
    "# print(FC2_Weight*32768)\n",
    "# print(FC1_Weight[0][0]*32768)\n",
    "print(np.max(FC1_Weight), np.min(FC1_Weight))\n",
    "print(np.max(FC3_Weight), np.min(FC3_Weight))\n",
    "\n",
    "print(FC1_Weight.shape)\n",
    "print(FC3_Weight.shape)\n",
    "# xxxx\n",
    "with open(\"FC_FSDD_Weight.coe\", \"w\") as f:\n",
    "    f.writelines(\"MEMORY_INITIALIZATION_RADIX=16;\\n\")\n",
    "    f.writelines(\"MEMORY_INITIALIZATION_VECTOR=\\n\")\n",
    "    for i in range(96):\n",
    "        for j in range(1):\n",
    "            Weight = FC1_Weight[i, 64*j:64*j+64]\n",
    "            # print(Weight.shape)\n",
    "            for k in range(64):\n",
    "                # print(Weight.shape)\n",
    "                f.writelines(bin2hex_fan(Weight[63-k]))\n",
    "            f.writelines('\\n')\n",
    "\n",
    "\n",
    "    for i in range(64):\n",
    "        Weight = FC3_Weight[i]\n",
    "        for k in range(54):\n",
    "        # for k in range(62):\n",
    "            f.writelines('0000')\n",
    "        for k in range(10):\n",
    "        # for k in range(2):\n",
    "            f.writelines(bin2hex_fan(Weight[9-k]))\n",
    "        #     f.writelines(bin2hex_fan(Weight[1 - k]))\n",
    "\n",
    "        f.writelines('\\n')"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-05T12:03:00.753862Z",
     "iopub.execute_input": "2025-11-05T12:03:00.754300Z",
     "iopub.status.idle": "2025-11-05T12:03:00.878073Z",
     "shell.execute_reply.started": "2025-11-05T12:03:00.754274Z",
     "shell.execute_reply": "2025-11-05T12:03:00.877136Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "1.4175415 -1.999939\n1.2242432 -1.999939\n(96, 64)\n(64, 10)\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "source": [
    "model = LSGRU(input_size=32, hidden_size=64, output_size=10).to(device)\n",
    "model.load_state_dict(torch.load(\"/kaggle/working/models/best_model.pth\"))\n",
    "\n",
    "for inputs, labels in test_loader:\n",
    "    inputs = inputs.squeeze(dim=1) \n",
    "    # print(inputs.shape)\n",
    "    # print(labels.shape)\n",
    "    # print(inputs[0][0])\n",
    "    # print(labels)\n",
    "    one_input = inputs[3].unsqueeze(dim=0) \n",
    "    one_label = labels[3]\n",
    "    print(one_input.shape)\n",
    "    print(one_label)\n",
    "    # xx\n",
    "    one_output = model(one_input)\n",
    "    one_output = torch.clamp(one_output, min= 0)\n",
    "    n = float(2 ** 3)\n",
    "    one_output = torch.clip(torch.round(one_output * n) / n , 0, 14.875)\n",
    "    print(one_output)\n",
    "    print(one_output*16384)\n",
    "    print(one_output*8)\n",
    "    _, predicted = torch.max(one_output, 1)\n",
    "    print(predicted)\n",
    "    break"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-05T12:04:04.428792Z",
     "iopub.execute_input": "2025-11-05T12:04:04.429190Z",
     "iopub.status.idle": "2025-11-05T12:04:04.537423Z",
     "shell.execute_reply.started": "2025-11-05T12:04:04.429157Z",
     "shell.execute_reply": "2025-11-05T12:04:04.536376Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "torch.Size([1, 16, 32])\ntensor(2)\ntensor([[0.7500, 0.7500, 3.1250, 0.8750, 0.0000, 0.0000, 0.3750, 0.6250, 0.0000,\n         0.0000]], grad_fn=<ClampBackward1>)\ntensor([[12288., 12288., 51200., 14336.,     0.,     0.,  6144., 10240.,     0.,\n             0.]], grad_fn=<MulBackward0>)\ntensor([[ 6.,  6., 25.,  7.,  0.,  0.,  3.,  5.,  0.,  0.]],\n       grad_fn=<MulBackward0>)\ntensor([2])\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "source": [
    "test_Data = torch.tensor([[0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
    "         0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "         1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1.,\n",
    "         0., 1., 0., 0., 0., 1., 0., 0., 0., 0.]])\n",
    "\n",
    "Get_HexData(test_Data)"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-05T09:18:04.452152Z",
     "iopub.execute_input": "2025-11-05T09:18:04.452474Z",
     "iopub.status.idle": "2025-11-05T09:18:04.462129Z",
     "shell.execute_reply.started": "2025-11-05T09:18:04.452454Z",
     "shell.execute_reply": "2025-11-05T09:18:04.460871Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "倒序后的16进制数据: ['0', '8', 'B', '0', '8', '1', '5', '0', '0', '1', '7', '0', '2', '0', '8', '6']\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "num_index = int(3)\n",
    "\n",
    "file_path = \"/kaggle/working/save_dir/FSDD_test_data.npy\"\n",
    "label_file_path = \"/kaggle/working/save_dir/FSDD_test_label.npy\"\n",
    "data = np.load(file_path)\n",
    "reshaped_data = data\n",
    "label = np.load(label_file_path).reshape(300)\n",
    "print(label.shape)\n",
    "print(reshaped_data.shape)\n",
    "print(label[num_index])\n",
    "One_data_list = reshaped_data[num_index]\n",
    "One_target = label[num_index]\n",
    "int_one_data = One_data_list.reshape((1, 512)).astype(\"int\")\n",
    "print(int_one_data)\n",
    "int_list = [int(re.sub(r'[\\[\\]]', '', str(int_one_data[0][i: i + 8])).replace(\" \", \"\"), 2) for i in range(0, len(int_one_data[0]), 8)]\n",
    "np_array = np.array(int_list).reshape((1, len(int_list)))\n",
    "print(np_array)\n",
    "# xxxx\n",
    "data_len = len(int_list)\n",
    "print(data_len)\n",
    "os.makedirs(\"txt_file\", exist_ok=True)\n",
    "print(str(int(One_target)))\n",
    "\n",
    "print('txt_file/' + str(int(One_target)) + '.coe')\n",
    "# xx\n",
    "with open('txt_file/' + str(int(One_target)) + '.coe', 'w') as f:\n",
    "    # f.writelines(\"initial begin\\n\")\n",
    "    f.writelines(\"MEMORY_INITIALIZATION_RADIX=16;\\n\")\n",
    "    f.writelines(\"MEMORY_INITIALIZATION_VECTOR=\\n\")\n",
    "    for i in range(data_len):\n",
    "        # a = hex(data[0][i]).replace('0x','')\n",
    "        # print(a)\n",
    "        # f.writelines(\"image1[{0}] = {1};\\n\".format(i,data[0][i]))\n",
    "        x = hex(int_list[i]).replace('0x', '')\n",
    "        f.writelines('0' * (2 - len(x)) + x + '\\n')\n",
    "\n"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-05T12:04:19.303843Z",
     "iopub.execute_input": "2025-11-05T12:04:19.304748Z",
     "iopub.status.idle": "2025-11-05T12:04:19.325145Z",
     "shell.execute_reply.started": "2025-11-05T12:04:19.304714Z",
     "shell.execute_reply": "2025-11-05T12:04:19.323770Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "(300,)\n(300, 16, 32)\n2\n[[0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 1 0 0 0 0\n  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0\n  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n  0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n  0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n  0 0 0 0 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n  0 1 0 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1\n  0 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1\n  0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 0 1 0 1 0 0 0 1\n  1 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0 1 0 1 0 1 0 0 0 1 1 0 1 1\n  1 1 1 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1\n  1 1 0 1 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 1 1 1 1 0 1\n  0 0 0 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 0 0\n  0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 0 1\n  0 1 0 1 0 0 0 1]]\n[[  0  84   4  87   0   0   0  21   0   0   0  93   0   0   0  93   0  16\n    0  93   0   0   0  93   0   0   0  85  11 244   0  85  11 253   0  85\n   27 253  65 213  27 253  81 213  27 253  69  85  19 253  69 213  18 253\n    5 209  54 253   5 209   6 253   5  81]]\n64\n2\ntxt_file/2.coe\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 35
  }
 ]
}
